{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f0adfa-d2b1-4b1b-80bc-fe87e51ede18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/Jabasingh Daniel/Downloads/cobol_dataset2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a810371-2271-4418-8ff9-f5e6e3bc8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_cobol = Tokenizer(filters='')\n",
    "tokenizer_cobol.fit_on_texts(df['cobol code'])\n",
    "tokenizer_summary = Tokenizer(filters='')\n",
    "tokenizer_summary.fit_on_texts(df['summary'])\n",
    "\n",
    "# Define the vocabulary sizes\n",
    "vocab_size_cobol = len(tokenizer_cobol.word_index) + 1\n",
    "vocab_size_summary = len(tokenizer_summary.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384ae868-b97c-495f-960a-9e973f201d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer_cobol.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fd92beb-61b5-4abc-9d00-9363f941761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer_summary.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c073084-6973-4342-8104-56f7b8e8a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input-output pairs\n",
    "X = tokenizer_cobol.texts_to_sequences(df['cobol code'])\n",
    "Y = tokenizer_summary.texts_to_sequences(df['summary'])\n",
    "# Pad sequences\n",
    "max_len_cobol = max([len(seq) for seq in X])\n",
    "max_len_summary = max([len(seq) for seq in Y])\n",
    "\n",
    "X = pad_sequences(X, maxlen=max_len_cobol, padding='post')\n",
    "Y = pad_sequences(Y, maxlen=max_len_summary, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92008beb-2312-4135-a738-91f2683f3b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea8de850-834c-4091-aca5-ab14f197fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the target sequences for training\n",
    "Y_input = Y[:, :-1]\n",
    "Y_output = Y[:, 1:]\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "latent_dim = 512\n",
    "\n",
    "encoder_inputs = Input(shape=(max_len_cobol,))\n",
    "encoder_embedding = Embedding(vocab_size_cobol, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(max_len_summary-1,))\n",
    "decoder_embedding = Embedding(vocab_size_summary, latent_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_summary, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dccaff73-68bb-4407-8941-01d777ec0cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 982ms/step - accuracy: 0.8172 - loss: 4.1830 - val_accuracy: 0.8501 - val_loss: 5.0072\n",
      "Epoch 2/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 781ms/step - accuracy: 0.8323 - loss: 3.9890 - val_accuracy: 0.8554 - val_loss: 5.0042\n",
      "Epoch 3/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 825ms/step - accuracy: 0.7726 - loss: 3.8915 - val_accuracy: 0.8566 - val_loss: 4.9690\n",
      "Epoch 4/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 839ms/step - accuracy: 0.8280 - loss: 3.7698 - val_accuracy: 0.3632 - val_loss: 4.9782\n",
      "Epoch 5/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 900ms/step - accuracy: 0.6177 - loss: 3.6225 - val_accuracy: 0.2505 - val_loss: 4.8996\n",
      "Epoch 6/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 797ms/step - accuracy: 0.4522 - loss: 3.5149 - val_accuracy: 0.2432 - val_loss: 4.9655\n",
      "Epoch 7/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 806ms/step - accuracy: 0.2899 - loss: 3.4606 - val_accuracy: 0.3164 - val_loss: 4.8946\n",
      "Epoch 8/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 801ms/step - accuracy: 0.3543 - loss: 3.4002 - val_accuracy: 0.1976 - val_loss: 5.0566\n",
      "Epoch 9/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 820ms/step - accuracy: 0.2688 - loss: 3.3076 - val_accuracy: 0.3244 - val_loss: 4.8246\n",
      "Epoch 10/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 834ms/step - accuracy: 0.3600 - loss: 3.0557 - val_accuracy: 0.3992 - val_loss: 4.8704\n",
      "Epoch 11/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 789ms/step - accuracy: 0.3718 - loss: 2.9419 - val_accuracy: 0.3341 - val_loss: 4.8246\n",
      "Epoch 12/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 808ms/step - accuracy: 0.3137 - loss: 2.8927 - val_accuracy: 0.3196 - val_loss: 4.7975\n",
      "Epoch 13/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 803ms/step - accuracy: 0.3588 - loss: 2.7747 - val_accuracy: 0.5717 - val_loss: 4.7901\n",
      "Epoch 14/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 892ms/step - accuracy: 0.5040 - loss: 2.5229 - val_accuracy: 0.6493 - val_loss: 4.8201\n",
      "Epoch 15/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 818ms/step - accuracy: 0.5299 - loss: 2.4527 - val_accuracy: 0.4364 - val_loss: 4.7502\n",
      "Epoch 16/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 838ms/step - accuracy: 0.3985 - loss: 2.4949 - val_accuracy: 0.7471 - val_loss: 4.8203\n",
      "Epoch 17/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 847ms/step - accuracy: 0.4968 - loss: 2.3074 - val_accuracy: 0.4251 - val_loss: 4.7739\n",
      "Epoch 18/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 844ms/step - accuracy: 0.4256 - loss: 2.2899 - val_accuracy: 0.5972 - val_loss: 4.7916\n",
      "Epoch 19/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 794ms/step - accuracy: 0.5832 - loss: 2.0376 - val_accuracy: 0.6331 - val_loss: 4.7986\n",
      "Epoch 20/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 809ms/step - accuracy: 0.6246 - loss: 2.0311 - val_accuracy: 0.2448 - val_loss: 4.8378\n",
      "Epoch 21/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 844ms/step - accuracy: 0.3610 - loss: 1.8667 - val_accuracy: 0.5539 - val_loss: 4.7570\n",
      "Epoch 22/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 843ms/step - accuracy: 0.4875 - loss: 1.8798 - val_accuracy: 0.4267 - val_loss: 4.8349\n",
      "Epoch 23/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 898ms/step - accuracy: 0.4994 - loss: 1.6991 - val_accuracy: 0.2917 - val_loss: 4.7847\n",
      "Epoch 24/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 920ms/step - accuracy: 0.4312 - loss: 1.7735 - val_accuracy: 0.7644 - val_loss: 4.8207\n",
      "Epoch 25/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 998ms/step - accuracy: 0.6067 - loss: 1.6810 - val_accuracy: 0.4404 - val_loss: 4.8770\n",
      "Epoch 26/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.5122 - loss: 1.6383 - val_accuracy: 0.7152 - val_loss: 4.8561\n",
      "Epoch 27/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.5903 - loss: 1.5480 - val_accuracy: 0.5180 - val_loss: 4.8974\n",
      "Epoch 28/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.6051 - loss: 1.4390 - val_accuracy: 0.3212 - val_loss: 4.8877\n",
      "Epoch 29/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 848ms/step - accuracy: 0.3945 - loss: 1.4128 - val_accuracy: 0.6174 - val_loss: 4.8480\n",
      "Epoch 30/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 787ms/step - accuracy: 0.5514 - loss: 1.3865 - val_accuracy: 0.6117 - val_loss: 4.9297\n",
      "Epoch 31/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 838ms/step - accuracy: 0.6125 - loss: 1.3005 - val_accuracy: 0.6109 - val_loss: 4.9144\n",
      "Epoch 32/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 833ms/step - accuracy: 0.6640 - loss: 1.2283 - val_accuracy: 0.5830 - val_loss: 4.9500\n",
      "Epoch 33/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 791ms/step - accuracy: 0.6380 - loss: 1.2077 - val_accuracy: 0.7354 - val_loss: 4.8939\n",
      "Epoch 34/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 981ms/step - accuracy: 0.6591 - loss: 1.1881 - val_accuracy: 0.5386 - val_loss: 4.9871\n",
      "Epoch 35/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 825ms/step - accuracy: 0.5969 - loss: 1.0763 - val_accuracy: 0.7818 - val_loss: 5.0168\n",
      "Epoch 36/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 816ms/step - accuracy: 0.7216 - loss: 1.0656 - val_accuracy: 0.5863 - val_loss: 4.9684\n",
      "Epoch 37/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 793ms/step - accuracy: 0.6829 - loss: 1.0182 - val_accuracy: 0.7479 - val_loss: 5.0046\n",
      "Epoch 38/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 826ms/step - accuracy: 0.6657 - loss: 0.9840 - val_accuracy: 0.5402 - val_loss: 4.9599\n",
      "Epoch 39/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 823ms/step - accuracy: 0.6363 - loss: 0.9205 - val_accuracy: 0.6380 - val_loss: 5.0279\n",
      "Epoch 40/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 806ms/step - accuracy: 0.6528 - loss: 0.8990 - val_accuracy: 0.5244 - val_loss: 5.0300\n",
      "Epoch 41/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 818ms/step - accuracy: 0.6606 - loss: 0.8325 - val_accuracy: 0.7337 - val_loss: 5.1183\n",
      "Epoch 42/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 806ms/step - accuracy: 0.7563 - loss: 0.8692 - val_accuracy: 0.6970 - val_loss: 5.0325\n",
      "Epoch 43/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 841ms/step - accuracy: 0.7068 - loss: 0.7524 - val_accuracy: 0.5725 - val_loss: 5.1137\n",
      "Epoch 44/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 825ms/step - accuracy: 0.6785 - loss: 0.7927 - val_accuracy: 0.7612 - val_loss: 5.1317\n",
      "Epoch 45/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 858ms/step - accuracy: 0.6935 - loss: 0.7922 - val_accuracy: 0.6356 - val_loss: 5.1205\n",
      "Epoch 46/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 856ms/step - accuracy: 0.6863 - loss: 0.7502 - val_accuracy: 0.7927 - val_loss: 5.1707\n",
      "Epoch 47/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 802ms/step - accuracy: 0.6887 - loss: 0.7539 - val_accuracy: 0.6509 - val_loss: 5.1358\n",
      "Epoch 48/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 846ms/step - accuracy: 0.6816 - loss: 0.6793 - val_accuracy: 0.6626 - val_loss: 5.1064\n",
      "Epoch 49/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 806ms/step - accuracy: 0.6907 - loss: 0.6805 - val_accuracy: 0.6562 - val_loss: 5.1189\n",
      "Epoch 50/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 839ms/step - accuracy: 0.7021 - loss: 0.6865 - val_accuracy: 0.6275 - val_loss: 5.1852\n",
      "Epoch 51/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 825ms/step - accuracy: 0.6334 - loss: 0.6369 - val_accuracy: 0.5770 - val_loss: 5.2362\n",
      "Epoch 52/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 857ms/step - accuracy: 0.6749 - loss: 0.5618 - val_accuracy: 0.6646 - val_loss: 5.1878\n",
      "Epoch 53/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 836ms/step - accuracy: 0.7652 - loss: 0.5608 - val_accuracy: 0.5996 - val_loss: 5.1783\n",
      "Epoch 54/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 858ms/step - accuracy: 0.7074 - loss: 0.5593 - val_accuracy: 0.7552 - val_loss: 5.2132\n",
      "Epoch 55/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 834ms/step - accuracy: 0.6653 - loss: 0.5862 - val_accuracy: 0.5216 - val_loss: 5.2156\n",
      "Epoch 56/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 840ms/step - accuracy: 0.6359 - loss: 0.5214 - val_accuracy: 0.7143 - val_loss: 5.2218\n",
      "Epoch 57/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 825ms/step - accuracy: 0.7118 - loss: 0.5061 - val_accuracy: 0.5636 - val_loss: 5.2803\n",
      "Epoch 58/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 854ms/step - accuracy: 0.6520 - loss: 0.5019 - val_accuracy: 0.7038 - val_loss: 5.2868\n",
      "Epoch 59/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 834ms/step - accuracy: 0.7374 - loss: 0.4786 - val_accuracy: 0.7911 - val_loss: 5.3131\n",
      "Epoch 60/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 916ms/step - accuracy: 0.7074 - loss: 0.4658 - val_accuracy: 0.6481 - val_loss: 5.3510\n",
      "Epoch 61/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 833ms/step - accuracy: 0.7325 - loss: 0.4636 - val_accuracy: 0.8275 - val_loss: 5.4050\n",
      "Epoch 62/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 790ms/step - accuracy: 0.7686 - loss: 0.4928 - val_accuracy: 0.6158 - val_loss: 5.3099\n",
      "Epoch 63/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 853ms/step - accuracy: 0.7055 - loss: 0.4359 - val_accuracy: 0.7947 - val_loss: 5.3299\n",
      "Epoch 64/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 809ms/step - accuracy: 0.7989 - loss: 0.4413 - val_accuracy: 0.7374 - val_loss: 5.3333\n",
      "Epoch 65/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 833ms/step - accuracy: 0.7944 - loss: 0.4548 - val_accuracy: 0.7943 - val_loss: 5.3831\n",
      "Epoch 66/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 896ms/step - accuracy: 0.7270 - loss: 0.4378 - val_accuracy: 0.7572 - val_loss: 5.3867\n",
      "Epoch 67/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 957ms/step - accuracy: 0.7926 - loss: 0.4165 - val_accuracy: 0.8279 - val_loss: 5.4198\n",
      "Epoch 68/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 840ms/step - accuracy: 0.8185 - loss: 0.3886 - val_accuracy: 0.7741 - val_loss: 5.4093\n",
      "Epoch 69/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 789ms/step - accuracy: 0.7918 - loss: 0.3957 - val_accuracy: 0.8238 - val_loss: 5.3931\n",
      "Epoch 70/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 836ms/step - accuracy: 0.8322 - loss: 0.3941 - val_accuracy: 0.8077 - val_loss: 5.3833\n",
      "Epoch 71/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 852ms/step - accuracy: 0.8277 - loss: 0.3681 - val_accuracy: 0.8226 - val_loss: 5.4691\n",
      "Epoch 72/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.8383 - loss: 0.4020 - val_accuracy: 0.8384 - val_loss: 5.4644\n",
      "Epoch 73/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 896ms/step - accuracy: 0.8650 - loss: 0.3812 - val_accuracy: 0.8069 - val_loss: 5.4265\n",
      "Epoch 74/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 915ms/step - accuracy: 0.8368 - loss: 0.3725 - val_accuracy: 0.8440 - val_loss: 5.4608\n",
      "Epoch 75/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 962ms/step - accuracy: 0.8992 - loss: 0.3426 - val_accuracy: 0.8343 - val_loss: 5.5115\n",
      "Epoch 76/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 987ms/step - accuracy: 0.8444 - loss: 0.3209 - val_accuracy: 0.7883 - val_loss: 5.5092\n",
      "Epoch 77/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 869ms/step - accuracy: 0.7918 - loss: 0.3536 - val_accuracy: 0.7600 - val_loss: 5.4915\n",
      "Epoch 78/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 842ms/step - accuracy: 0.7952 - loss: 0.3318 - val_accuracy: 0.7337 - val_loss: 5.5095\n",
      "Epoch 79/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 809ms/step - accuracy: 0.7607 - loss: 0.3145 - val_accuracy: 0.7584 - val_loss: 5.5106\n",
      "Epoch 80/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 834ms/step - accuracy: 0.7739 - loss: 0.3075 - val_accuracy: 0.8311 - val_loss: 5.6012\n",
      "Epoch 81/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 806ms/step - accuracy: 0.8323 - loss: 0.2879 - val_accuracy: 0.8251 - val_loss: 5.5535\n",
      "Epoch 82/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 854ms/step - accuracy: 0.8669 - loss: 0.3204 - val_accuracy: 0.8424 - val_loss: 5.5125\n",
      "Epoch 83/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 871ms/step - accuracy: 0.8343 - loss: 0.2920 - val_accuracy: 0.8352 - val_loss: 5.5160\n",
      "Epoch 84/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 863ms/step - accuracy: 0.8462 - loss: 0.2777 - val_accuracy: 0.8440 - val_loss: 5.5439\n",
      "Epoch 85/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 822ms/step - accuracy: 0.8679 - loss: 0.2772 - val_accuracy: 0.8428 - val_loss: 5.5915\n",
      "Epoch 86/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 829ms/step - accuracy: 0.8814 - loss: 0.2767 - val_accuracy: 0.8384 - val_loss: 5.5281\n",
      "Epoch 87/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 828ms/step - accuracy: 0.9066 - loss: 0.2635 - val_accuracy: 0.8424 - val_loss: 5.6126\n",
      "Epoch 88/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 801ms/step - accuracy: 0.8462 - loss: 0.2759 - val_accuracy: 0.8436 - val_loss: 5.5781\n",
      "Epoch 89/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 843ms/step - accuracy: 0.8422 - loss: 0.2913 - val_accuracy: 0.8404 - val_loss: 5.5695\n",
      "Epoch 90/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 839ms/step - accuracy: 0.8645 - loss: 0.2622 - val_accuracy: 0.8408 - val_loss: 5.6146\n",
      "Epoch 91/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 854ms/step - accuracy: 0.8899 - loss: 0.2506 - val_accuracy: 0.8428 - val_loss: 5.5918\n",
      "Epoch 92/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 830ms/step - accuracy: 0.8641 - loss: 0.2426 - val_accuracy: 0.8448 - val_loss: 5.6040\n",
      "Epoch 93/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 808ms/step - accuracy: 0.8815 - loss: 0.2453 - val_accuracy: 0.8384 - val_loss: 5.6168\n",
      "Epoch 94/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 831ms/step - accuracy: 0.8822 - loss: 0.2856 - val_accuracy: 0.8065 - val_loss: 5.6418\n",
      "Epoch 95/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 807ms/step - accuracy: 0.8225 - loss: 0.2489 - val_accuracy: 0.8356 - val_loss: 5.6272\n",
      "Epoch 96/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 860ms/step - accuracy: 0.8489 - loss: 0.2386 - val_accuracy: 0.8432 - val_loss: 5.6610\n",
      "Epoch 97/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 827ms/step - accuracy: 0.8768 - loss: 0.2264 - val_accuracy: 0.8444 - val_loss: 5.6535\n",
      "Epoch 98/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 886ms/step - accuracy: 0.8857 - loss: 0.2396 - val_accuracy: 0.8469 - val_loss: 5.6723\n",
      "Epoch 99/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 842ms/step - accuracy: 0.8941 - loss: 0.2055 - val_accuracy: 0.8436 - val_loss: 5.6170\n",
      "Epoch 100/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 925ms/step - accuracy: 0.8843 - loss: 0.2357 - val_accuracy: 0.8404 - val_loss: 5.6695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x225072d4a90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit([X, Y[:,:-1]], Y.reshape(Y.shape[0], Y.shape[1], 1)[:,1:],batch_size=20, epochs=100, validation_split=0.2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ee7f066-6061-42c5-a641-24e4acd5e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('seq2seq_model_D2.keras') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c711bf44-421b-48b3-b478-638adb28ff87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 329ms/step\n",
      "Sequence Accuracy: 53.85%\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming your test data is loaded and preprocessed similarly to the training data\n",
    "df_test = pd.read_csv(\"C:/Users/Jabasingh Daniel/Downloads/cobol_dataset2.csv\")\n",
    "X_test = tokenizer_cobol.texts_to_sequences(df_test['cobol code'])\n",
    "Y_test = tokenizer_summary.texts_to_sequences(df_test['summary'])\n",
    "\n",
    "X_test = pad_sequences(X_test, maxlen=max_len_cobol, padding='post')\n",
    "Y_test = pad_sequences(Y_test, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"C:/Users/Jabasingh Daniel/Desktop/EGDK/modeling/seq2seq_model_D2.keras\")\n",
    "\n",
    "# Function to calculate sequence accuracy\n",
    "def sequence_accuracy(y_true, y_pred):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for true_seq, pred_seq in zip(y_true, y_pred):\n",
    "        true_seq = true_seq[true_seq != 0]  # Remove padding\n",
    "        pred_seq = pred_seq[:len(true_seq)]  # Truncate to the length of the true sequence\n",
    "        if np.array_equal(true_seq, pred_seq):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total\n",
    "\n",
    "# Generate predictions\n",
    "Y_pred = model.predict([X_test, Y_test[:, :-1]])\n",
    "\n",
    "# Convert predictions to sequences of token ids\n",
    "Y_pred_sequences = np.argmax(Y_pred, axis=-1)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = sequence_accuracy(Y_test[:, 1:], Y_pred_sequences)\n",
    "print(f'Sequence Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8257ae5-0cfb-4c88-a30d-f5cca7be0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"C:/Users/Jabasingh Daniel/Desktop/EGDK/modeling/seq2seq_model_D2.keras\")\n",
    "\n",
    "def preprocess_input(cobol_code, tokenizer_cobol, max_len_cobol):\n",
    "    # Tokenize the input COBOL code\n",
    "    sequence = tokenizer_cobol.texts_to_sequences([cobol_code])\n",
    "    # Pad the sequence\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len_cobol, padding='post')\n",
    "    return padded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79da8e92-57c8-44e1-b84d-327d6b895118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_summary(cobol_code, model, tokenizer_cobol, tokenizer_summary, max_len_cobol, max_len_summary):\n",
    "    # Preprocess the input COBOL code\n",
    "    input_seq = preprocess_input(cobol_code, tokenizer_cobol, max_len_cobol)\n",
    "    \n",
    "    # Initialize the decoder input\n",
    "    decoder_input = np.zeros((1, max_len_summary - 1))\n",
    "    \n",
    "    # Predict the summary\n",
    "    for i in range(max_len_summary - 1):\n",
    "        output_tokens = model.predict([input_seq, decoder_input])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, i, :])\n",
    "        decoder_input[0, i] = sampled_token_index\n",
    "        \n",
    "        # Stop if a zero token (padding) is predicted, indicating no more meaningful tokens\n",
    "        if sampled_token_index == 0:\n",
    "            break\n",
    "    \n",
    "    # Convert token indices back to words\n",
    "    predicted_summary = []\n",
    "    for token in decoder_input[0]:\n",
    "        if token == 0:\n",
    "            continue\n",
    "        word = tokenizer_summary.index_word.get(token, '')\n",
    "        predicted_summary.append(word)\n",
    "    \n",
    "    return ' '.join(predicted_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee1495e2-d8ad-4bd7-be39-113019666b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Summary: {price} to\n"
     ]
    }
   ],
   "source": [
    "# Define your COBOL code to be summarized\n",
    "cobol_code_example =\"01 {TOTAL} PIC 9(5) VALUE 0. 01 {PRICE} PIC 9(3) VALUE 100. \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict the summary\n",
    "summary = predict_summary(cobol_code_example, model, tokenizer_cobol, tokenizer_summary, max_len_cobol, max_len_summary)\n",
    "print(f'Summary: {summary}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a3cd9-1b24-46fd-ba9f-13bcbc42bde6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aae4bd-5c2c-4805-9e6a-a25966fd4f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
