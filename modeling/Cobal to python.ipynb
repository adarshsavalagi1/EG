{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f0adfa-d2b1-4b1b-80bc-fe87e51ede18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/Jabasingh Daniel/Desktop/EGDK/Dataset/cob_py.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c171cdb-3ba8-4fb3-ac19-34c87d0cd471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['python_code'] = df['python_code'].astype(str)\n",
    "summary = df['python_code'].values\n",
    "summary = ['<start> ' + text + ' <end>' for text in summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6eb9fd9-22dd-4ea2-a749-af51b60f6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e8573f5-46ff-4134-ab4b-f665964e6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with the 'summary' column\n",
    "df['python_code'] = '<start> ' + df['python_code'] + ' <end>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a810371-2271-4418-8ff9-f5e6e3bc8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_cobol = Tokenizer(filters='')\n",
    "tokenizer_cobol.fit_on_texts(df['cobol_code'])\n",
    "tokenizer_summary = Tokenizer(filters='')\n",
    "tokenizer_summary.fit_on_texts(df['python_code'])\n",
    "\n",
    "# Define the vocabulary sizes\n",
    "vocab_size_cobol = len(tokenizer_cobol.word_index) + 1\n",
    "vocab_size_summary = len(tokenizer_summary.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "384ae868-b97c-495f-960a-9e973f201d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer_cobol.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fd92beb-61b5-4abc-9d00-9363f941761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer_summary.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c073084-6973-4342-8104-56f7b8e8a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input-output pairs\n",
    "X = tokenizer_cobol.texts_to_sequences(df['cobol_code'])\n",
    "Y = tokenizer_summary.texts_to_sequences(df['python_code'])\n",
    "# Pad sequences\n",
    "max_len_cobol = max([len(seq) for seq in X])\n",
    "max_len_summary = max([len(seq) for seq in Y])\n",
    "\n",
    "X = pad_sequences(X, maxlen=max_len_cobol, padding='post')\n",
    "Y = pad_sequences(Y, maxlen=max_len_summary, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92008beb-2312-4135-a738-91f2683f3b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea8de850-834c-4091-aca5-ab14f197fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the target sequencesY for training\n",
    "Y_input = Y[:, :-1]\n",
    "Y_output = Y[:, 1:]\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "latent_dim = 512\n",
    "\n",
    "encoder_inputs = Input(shape=(max_len_cobol,))\n",
    "encoder_embedding = Embedding(vocab_size_cobol, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(max_len_summary-1,))\n",
    "decoder_embedding = Embedding(vocab_size_summary, latent_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_summary, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dccaff73-68bb-4407-8941-01d777ec0cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.0421 - loss: 6.1786 - val_accuracy: 0.0390 - val_loss: 6.1652\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0476 - loss: 6.0731 - val_accuracy: 0.0223 - val_loss: 6.1040\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.0223 - loss: 5.8929 - val_accuracy: 0.0390 - val_loss: 5.9128\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0453 - loss: 5.3962 - val_accuracy: 0.0390 - val_loss: 6.0553\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0408 - loss: 5.2898 - val_accuracy: 0.0390 - val_loss: 6.1237\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0429 - loss: 5.1674 - val_accuracy: 0.0390 - val_loss: 6.2042\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.2375 - loss: 5.0734 - val_accuracy: 0.0390 - val_loss: 6.2624\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.0428 - loss: 4.9792 - val_accuracy: 0.0390 - val_loss: 6.3478\n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1917 - loss: 4.8911 - val_accuracy: 0.0390 - val_loss: 6.4299\n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0465 - loss: 4.9412 - val_accuracy: 0.0390 - val_loss: 6.5139\n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0497 - loss: 4.8060 - val_accuracy: 0.0575 - val_loss: 6.6340\n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0827 - loss: 4.6963 - val_accuracy: 0.0557 - val_loss: 6.6253\n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.1038 - loss: 4.7788 - val_accuracy: 0.0519 - val_loss: 6.7326\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.1809 - loss: 4.6911 - val_accuracy: 0.0557 - val_loss: 6.7387\n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.1030 - loss: 4.6192 - val_accuracy: 0.0427 - val_loss: 6.7740\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0466 - loss: 4.6526 - val_accuracy: 0.3618 - val_loss: 6.8439\n",
      "Epoch 17/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.4060 - loss: 4.6335 - val_accuracy: 0.3284 - val_loss: 6.8605\n",
      "Epoch 18/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.4237 - loss: 4.5614 - val_accuracy: 0.0631 - val_loss: 6.9787\n",
      "Epoch 19/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.2035 - loss: 4.6967 - val_accuracy: 0.3432 - val_loss: 6.9892\n",
      "Epoch 20/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.5229 - loss: 4.3794 - val_accuracy: 0.0464 - val_loss: 6.9648\n",
      "Epoch 21/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1150 - loss: 4.4441 - val_accuracy: 0.3265 - val_loss: 7.1960\n",
      "Epoch 22/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.3005 - loss: 4.6584 - val_accuracy: 0.2579 - val_loss: 7.1091\n",
      "Epoch 23/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.2547 - loss: 4.4210 - val_accuracy: 0.3228 - val_loss: 7.1951\n",
      "Epoch 24/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.4932 - loss: 4.4168 - val_accuracy: 0.0594 - val_loss: 7.1279\n",
      "Epoch 25/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.2487 - loss: 4.4380 - val_accuracy: 0.0464 - val_loss: 7.1818\n",
      "Epoch 26/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.0516 - loss: 4.2891 - val_accuracy: 0.0872 - val_loss: 7.1641\n",
      "Epoch 27/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.2145 - loss: 4.4930 - val_accuracy: 0.0519 - val_loss: 7.1754\n",
      "Epoch 28/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1285 - loss: 4.2757 - val_accuracy: 0.0575 - val_loss: 7.1076\n",
      "Epoch 29/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0578 - loss: 4.4744 - val_accuracy: 0.3006 - val_loss: 7.1947\n",
      "Epoch 30/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.4923 - loss: 4.2112 - val_accuracy: 0.0575 - val_loss: 7.2166\n",
      "Epoch 31/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0603 - loss: 4.3559 - val_accuracy: 0.0594 - val_loss: 7.2080\n",
      "Epoch 32/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1785 - loss: 4.2077 - val_accuracy: 0.0891 - val_loss: 7.3701\n",
      "Epoch 33/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0887 - loss: 4.3149 - val_accuracy: 0.0872 - val_loss: 7.1087\n",
      "Epoch 34/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.2581 - loss: 4.2369 - val_accuracy: 0.0612 - val_loss: 7.2490\n",
      "Epoch 35/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.0890 - loss: 4.2054 - val_accuracy: 0.1132 - val_loss: 7.2611\n",
      "Epoch 36/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1270 - loss: 4.3409 - val_accuracy: 0.0594 - val_loss: 7.3316\n",
      "Epoch 37/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1156 - loss: 4.0894 - val_accuracy: 0.0668 - val_loss: 7.3030\n",
      "Epoch 38/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.0756 - loss: 4.0945 - val_accuracy: 0.0779 - val_loss: 7.4276\n",
      "Epoch 39/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.0902 - loss: 4.2383 - val_accuracy: 0.1596 - val_loss: 7.3572\n",
      "Epoch 40/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 0.1470 - loss: 4.3044 - val_accuracy: 0.0724 - val_loss: 7.2505\n",
      "Epoch 41/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0810 - loss: 4.0302 - val_accuracy: 0.0742 - val_loss: 7.3471\n",
      "Epoch 42/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.0855 - loss: 4.0368 - val_accuracy: 0.0686 - val_loss: 7.2410\n",
      "Epoch 43/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.0795 - loss: 4.0097 - val_accuracy: 0.0705 - val_loss: 7.3363\n",
      "Epoch 44/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.0832 - loss: 4.0550 - val_accuracy: 0.0742 - val_loss: 7.3600\n",
      "Epoch 45/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0751 - loss: 3.8928 - val_accuracy: 0.0705 - val_loss: 7.3897\n",
      "Epoch 46/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0798 - loss: 3.9453 - val_accuracy: 0.0631 - val_loss: 7.2413\n",
      "Epoch 47/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0817 - loss: 3.8915 - val_accuracy: 0.0798 - val_loss: 7.3440\n",
      "Epoch 48/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0799 - loss: 4.0037 - val_accuracy: 0.0575 - val_loss: 7.3328\n",
      "Epoch 49/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0833 - loss: 3.7651 - val_accuracy: 0.0649 - val_loss: 7.3947\n",
      "Epoch 50/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.0729 - loss: 3.7790 - val_accuracy: 0.0538 - val_loss: 7.4344\n",
      "Epoch 51/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.0688 - loss: 3.7556 - val_accuracy: 0.0612 - val_loss: 7.2947\n",
      "Epoch 52/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0789 - loss: 3.7236 - val_accuracy: 0.0649 - val_loss: 7.4543\n",
      "Epoch 53/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.0905 - loss: 3.6086 - val_accuracy: 0.0575 - val_loss: 7.5006\n",
      "Epoch 54/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.0866 - loss: 3.5803 - val_accuracy: 0.0742 - val_loss: 7.4215\n",
      "Epoch 55/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0748 - loss: 3.9385 - val_accuracy: 0.0575 - val_loss: 7.4296\n",
      "Epoch 56/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.0763 - loss: 3.8494 - val_accuracy: 0.0649 - val_loss: 7.3201\n",
      "Epoch 57/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.0853 - loss: 3.5659 - val_accuracy: 0.0612 - val_loss: 7.3748\n",
      "Epoch 58/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0968 - loss: 3.4517 - val_accuracy: 0.0575 - val_loss: 7.3930\n",
      "Epoch 59/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0901 - loss: 3.5855 - val_accuracy: 0.0538 - val_loss: 7.5098\n",
      "Epoch 60/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0838 - loss: 3.4941 - val_accuracy: 0.0631 - val_loss: 7.4874\n",
      "Epoch 61/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0998 - loss: 3.3867 - val_accuracy: 0.0538 - val_loss: 7.5252\n",
      "Epoch 62/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.0911 - loss: 3.5433 - val_accuracy: 0.0668 - val_loss: 7.4143\n",
      "Epoch 63/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.0992 - loss: 3.2979 - val_accuracy: 0.0761 - val_loss: 7.4059\n",
      "Epoch 64/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1058 - loss: 3.2535 - val_accuracy: 0.0686 - val_loss: 7.4437\n",
      "Epoch 65/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.1014 - loss: 3.1767 - val_accuracy: 0.0575 - val_loss: 7.6736\n",
      "Epoch 66/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0979 - loss: 3.2231 - val_accuracy: 0.0705 - val_loss: 7.4913\n",
      "Epoch 67/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.1089 - loss: 3.2689 - val_accuracy: 0.0668 - val_loss: 7.5398\n",
      "Epoch 68/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0951 - loss: 3.2742 - val_accuracy: 0.0686 - val_loss: 7.4558\n",
      "Epoch 69/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1060 - loss: 3.0330 - val_accuracy: 0.0594 - val_loss: 7.6545\n",
      "Epoch 70/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1187 - loss: 2.9200 - val_accuracy: 0.0761 - val_loss: 7.4831\n",
      "Epoch 71/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.1244 - loss: 2.8565 - val_accuracy: 0.0631 - val_loss: 7.6273\n",
      "Epoch 72/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1272 - loss: 2.8385 - val_accuracy: 0.0668 - val_loss: 7.6329\n",
      "Epoch 73/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.1314 - loss: 2.7426 - val_accuracy: 0.0668 - val_loss: 7.6981\n",
      "Epoch 74/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.1164 - loss: 3.0666 - val_accuracy: 0.0742 - val_loss: 7.4222\n",
      "Epoch 75/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.1215 - loss: 2.9684 - val_accuracy: 0.0705 - val_loss: 7.5816\n",
      "Epoch 76/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1353 - loss: 2.6066 - val_accuracy: 0.0631 - val_loss: 7.6532\n",
      "Epoch 77/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.1561 - loss: 2.5225 - val_accuracy: 0.0724 - val_loss: 7.7936\n",
      "Epoch 78/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.1274 - loss: 2.6036 - val_accuracy: 0.0798 - val_loss: 7.6169\n",
      "Epoch 79/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1314 - loss: 2.5691 - val_accuracy: 0.0668 - val_loss: 7.7364\n",
      "Epoch 80/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1350 - loss: 2.4345 - val_accuracy: 0.0668 - val_loss: 7.6508\n",
      "Epoch 81/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.1680 - loss: 2.4005 - val_accuracy: 0.0686 - val_loss: 7.8588\n",
      "Epoch 82/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1488 - loss: 2.3947 - val_accuracy: 0.0557 - val_loss: 7.9580\n",
      "Epoch 83/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.1478 - loss: 2.5677 - val_accuracy: 0.0575 - val_loss: 7.9530\n",
      "Epoch 84/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.1520 - loss: 2.3948 - val_accuracy: 0.0891 - val_loss: 7.6269\n",
      "Epoch 85/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.1974 - loss: 2.1028 - val_accuracy: 0.0779 - val_loss: 7.8326\n",
      "Epoch 86/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1851 - loss: 2.1430 - val_accuracy: 0.0891 - val_loss: 7.9370\n",
      "Epoch 87/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1637 - loss: 2.1759 - val_accuracy: 0.0816 - val_loss: 7.6324\n",
      "Epoch 88/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1506 - loss: 2.4604 - val_accuracy: 0.0761 - val_loss: 7.7735\n",
      "Epoch 89/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.1935 - loss: 1.9665 - val_accuracy: 0.0853 - val_loss: 7.6416\n",
      "Epoch 90/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.2497 - loss: 1.8687 - val_accuracy: 0.0835 - val_loss: 7.8316\n",
      "Epoch 91/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.2177 - loss: 1.9512 - val_accuracy: 0.0835 - val_loss: 7.8450\n",
      "Epoch 92/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.2431 - loss: 1.7858 - val_accuracy: 0.0816 - val_loss: 7.8937\n",
      "Epoch 93/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.2345 - loss: 1.7771 - val_accuracy: 0.0965 - val_loss: 7.9065\n",
      "Epoch 94/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1565 - loss: 2.3559 - val_accuracy: 0.0872 - val_loss: 7.7157\n",
      "Epoch 95/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.2335 - loss: 1.7034 - val_accuracy: 0.0928 - val_loss: 7.7913\n",
      "Epoch 96/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.2620 - loss: 1.6250 - val_accuracy: 0.0835 - val_loss: 7.8335\n",
      "Epoch 97/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.2407 - loss: 1.6264 - val_accuracy: 0.0909 - val_loss: 7.9004\n",
      "Epoch 98/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.2287 - loss: 1.5729 - val_accuracy: 0.0946 - val_loss: 8.0137\n",
      "Epoch 99/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.2307 - loss: 1.7467 - val_accuracy: 0.0835 - val_loss: 7.9941\n",
      "Epoch 100/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.2511 - loss: 1.5475 - val_accuracy: 0.0909 - val_loss: 7.9733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e68def0810>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit([X, Y[:,:-1]], Y.reshape(Y.shape[0], Y.shape[1], 1)[:,1:],batch_size=10, epochs=100, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ee7f066-6061-42c5-a641-24e4acd5e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('seq2seq_model_cobo_py.keras') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c711bf44-421b-48b3-b478-638adb28ff87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=[[[8, 2, 9, 204, 10, 2, 3, 205, 206, 11, 12]], array([[2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])] (of type <class 'list'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_test)):\n\u001b[0;32m     64\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m X_test[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 65\u001b[0m     predicted_summary \u001b[38;5;241m=\u001b[39m predict_summary(input_seq, model, tokenizer_summary, max_len_summary)\n\u001b[0;32m     66\u001b[0m     predicted_summaries\u001b[38;5;241m.\u001b[39mappend(predicted_summary)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Tokenize predicted summaries\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[49], line 32\u001b[0m, in \u001b[0;36mpredict_summary\u001b[1;34m(input_seq, model, tokenizer_summary, max_len_summary)\u001b[0m\n\u001b[0;32m     29\u001b[0m decoder_input[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m start_token\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_len_summary \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 32\u001b[0m     output_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([input_seq, decoder_input])\n\u001b[0;32m     33\u001b[0m     sampled_token_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output_tokens[\u001b[38;5;241m0\u001b[39m, i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     34\u001b[0m     decoder_input[\u001b[38;5;241m0\u001b[39m, i] \u001b[38;5;241m=\u001b[39m sampled_token_index\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py:120\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[1;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized data type: x=[[[8, 2, 9, 204, 10, 2, 3, 205, 206, 11, 12]], array([[2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])] (of type <class 'list'>)"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming your test data is loaded and preprocessed similarly to the training data\n",
    "df_test = pd.read_csv(\"C:/Users/Jabasingh Daniel/Desktop/EGDK/Dataset/cob_py.csv\")\n",
    "df_test = df_test.fillna('')\n",
    "\n",
    "# Ensure all values are strings\n",
    "df_test['cobol_code'] = df_test['cobol_code'].astype(str)\n",
    "df_test['python_code'] = df_test['python_code'].astype(str)\n",
    "\n",
    "X_test = tokenizer_cobol.texts_to_sequences(df_test['cobol_code'])\n",
    "Y_test = tokenizer_summary.texts_to_sequences(df_test['python_code'])\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"C:/Users/Jabasingh Daniel/Desktop/EGDK/modeling/seq2seq_model_cobo_py.keras\")\n",
    "\n",
    "\n",
    "# Define start and end tokens\n",
    "start_token = tokenizer_summary.word_index.get('<start>', 0)\n",
    "end_token = tokenizer_summary.word_index.get('<end>', 0)\n",
    "\n",
    "# Function to predict summary for a single input sequence\n",
    "def predict_summary(input_seq, model, tokenizer_summary, max_len_summary):\n",
    "    decoder_input = np.zeros((1, max_len_summary - 1))\n",
    "    decoder_input[0, 0] = start_token\n",
    "    \n",
    "    for i in range(1, max_len_summary - 1):\n",
    "        output_tokens = model.predict([input_seq, decoder_input])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, i-1, :])\n",
    "        decoder_input[0, i] = sampled_token_index\n",
    "        \n",
    "        if sampled_token_index == end_token:\n",
    "            break\n",
    "    \n",
    "    predicted_summary = []\n",
    "    for token in decoder_input[0]:\n",
    "        if token == 0:\n",
    "            continue\n",
    "        word = tokenizer_summary.index_word.get(token, '')\n",
    "        if word == '<end>':\n",
    "            break\n",
    "        predicted_summary.append(word)\n",
    "    \n",
    "    return ' '.join(predicted_summary)\n",
    "\n",
    "# Function to calculate sequence accuracy\n",
    "def sequence_accuracy(y_true, y_pred):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for true_seq, pred_seq in zip(y_true, y_pred):\n",
    "        true_seq = true_seq[true_seq != 0]  # Remove padding\n",
    "        pred_seq = pred_seq[:len(true_seq)]  # Truncate to the length of the true sequence\n",
    "        if np.array_equal(true_seq, pred_seq):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total\n",
    "\n",
    "predicted_summaries = []\n",
    "for i in range(len(X_test)):\n",
    "    input_seq = X_test[i:i+1]\n",
    "    predicted_summary = predict_summary(input_seq, model, tokenizer_summary, max_len_summary)\n",
    "    predicted_summaries.append(predicted_summary)\n",
    "\n",
    "# Tokenize predicted summaries\n",
    "Y_pred = tokenizer_summary.texts_to_sequences(predicted_summaries)\n",
    "Y_pred = pad_sequences(Y_pred, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = sequence_accuracy(Y_test[:, 1:], Y_pred_sequences)\n",
    "print(f'Sequence Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8257ae5-0cfb-4c88-a30d-f5cca7be0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"C:/Users/Jabasingh Daniel/Desktop/EGDK/modeling/seq2seq_model.keras\")\n",
    "\n",
    "def preprocess_input(cobol_code, tokenizer_cobol, max_len_cobol):\n",
    "    # Tokenize the input COBOL code\n",
    "    sequence = tokenizer_cobol.texts_to_sequences([cobol_code])\n",
    "    # Pad the sequence\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len_cobol, padding='post')\n",
    "    return padded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79da8e92-57c8-44e1-b84d-327d6b895118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_summary(cobol_code, model, tokenizer_cobol, tokenizer_summary, max_len_cobol, max_len_summary):\n",
    "    # Preprocess the input COBOL code\n",
    "    input_seq = preprocess_input(cobol_code, tokenizer_cobol, max_len_cobol)\n",
    "    \n",
    "    # Initialize the decoder input\n",
    "    decoder_input = np.zeros((1, max_len_summary - 1))\n",
    "    \n",
    "    # Predict the summary\n",
    "    for i in range(max_len_summary - 1):\n",
    "        output_tokens = model.predict([input_seq, decoder_input])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, i, :])\n",
    "        decoder_input[0, i] = sampled_token_index\n",
    "        \n",
    "        # Stop if a zero token (padding) is predicted, indicating no more meaningful tokens\n",
    "        if sampled_token_index == 0:\n",
    "            break\n",
    "    \n",
    "    # Convert token indices back to words\n",
    "    predicted_summary = []\n",
    "    for token in decoder_input[0]:\n",
    "        if token == 0:\n",
    "            continue\n",
    "        word = tokenizer_summary.index_word.get(token, '')\n",
    "        predicted_summary.append(word)\n",
    "    \n",
    "    return ' '.join(predicted_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1495e2-d8ad-4bd7-be39-113019666b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your COBOL code to be summarized\n",
    "cobol_code_example = \"SORT {DATASET} ON ASCENDING KEY {KEY1} USING {INPUT-FILE} GIVING {OUTPUT-FILE}.\"\n",
    "\n",
    "# Predict the summary\n",
    "summary = predict_summary(cobol_code_example, model, tokenizer_cobol, tokenizer_summary, max_len_cobol, max_len_summary)\n",
    "print(f'Summary: {summary}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e71a3cd9-1b24-46fd-ba9f-13bcbc42bde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cobol_code     0\n",
      "python_code    1\n",
      "dtype: int64\n",
      "                                           cobol_code python_code\n",
      "23  IDENTIFICATION DIVISION. PROGRAM-ID. PRG18. EN...         NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_test = pd.read_csv(\"C:/Users/Jabasingh Daniel/Desktop/EGDK/Dataset/cob_py.csv\")\n",
    "\n",
    "# Check for NaN values\n",
    "print(df_test.isna().sum())\n",
    "\n",
    "# Display rows with NaN values\n",
    "print(df_test[df_test.isna().any(axis=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad90d879-57cf-41e2-9551-a3e2f2077a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70c554bd-85b3-41ba-b829-b7029ac076a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Assuming tokenizer_cobol and tokenizer_summary are already fitted on your training data\n",
    "\n",
    "# Tokenize the COBOL code and Python code\n",
    "X_test = tokenizer_cobol.texts_to_sequences(df_test['cobol_code'])\n",
    "Y_test = tokenizer_summary.texts_to_sequences(df_test['python_code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026043e-eeaf-432b-a574-1cb36dd08213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
